{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "a_tensor_initialization",
   "id": "b0ae40719877ea5a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-23T06:11:00.465539Z",
     "start_time": "2024-09-23T06:11:00.364880Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu') #tensor는 기본적으로 float32로 생성된다.\n",
    "print(t1.dtype)   # >>> torch.float32 / t1의 데이터 타입을 알 수 있다.\n",
    "print(t1.device)  # >>> cpu / t1이 할당된 디바이스를 알 수 있다.\n",
    "print(t1.requires_grad)  # >>> False / 기울기 추적을 하는지 안 하는지 알 수 있다.\n",
    "print(t1.size())  # torch.Size([3]) / t1의 크기를 알 수 잇다.\n",
    "print(t1.shape)   # torch.Size([3]) / t1의 크기를 알 수 있다.\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu') # int64 타입의 텐서를 CPU에 생성한다.\n",
    "print(t2.dtype)  # >>> torch.int64 / 정수 리스트는 int64로 생성된다.\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim) # 텐서의 shape과 차원을 출력한다.\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],           # 차원이 일관되지 않아 발생한 에러다.\n",
    "    [[[1, 2, 3], [4, 5]]],           # 동일한 차원에 속하는 요소들의 길이가 일치해야 텐서를 구성할 수 있다.\n",
    "    [[[1, 2, 3], [4, 5]]],           # 따라서 [4,5]의 길이를 3으로 맞추거나, 다른 요소의 길이를 줄여야 한다.\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 99\u001B[0m\n\u001B[0;32m     91\u001B[0m a10 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001B[39;00m\n\u001B[0;32m     92\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     93\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     94\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     95\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     96\u001B[0m ])\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28mprint\u001B[39m(a10\u001B[38;5;241m.\u001B[39mshape, a10\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m---> 99\u001B[0m a11 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;00m\n\u001B[0;32m    100\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],           \u001B[38;5;66;03m# 차원이 일관되지 않아 발생한 에러다.\u001B[39;00m\n\u001B[0;32m    101\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],           \u001B[38;5;66;03m# 동일한 차원에 속하는 요소들의 길이가 일치해야 텐서를 구성할 수 있다.\u001B[39;00m\n\u001B[0;32m    102\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],           \u001B[38;5;66;03m# 따라서 [4,5]의 길이를 3으로 맞추거나, 다른 요소의 길이를 줄여야 한다.\u001B[39;00m\n\u001B[0;32m    103\u001B[0m     [[[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]]],\n\u001B[0;32m    104\u001B[0m ])\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* torch.Tensor와 torch.tensor의 차이\n",
    "    * torch.Tensor은 float32로 데이터 타입이 고정되어 있고, torch.tensor는 전달된 값의 타입에 따라 자동으로 자료형이 결정된다.\n",
    "* torch.size()와 torch.shape의 차이\n",
    "    * torch.size()는 함수처럼 호출하는 방식이고, torch.shape은 속성으로 바로 접근하는 방식이다. \n",
    "    * 하지만, 결국 둘은 동일한 정보를 반환한다."
   ],
   "id": "85a9e6e3c8e5651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b_tensor_initialization_copy",
   "id": "9557b516b22e658a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:11:16.391158Z",
     "start_time": "2024-09-23T06:11:16.363794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 리스트 l1을 사용해 torch.Tensor로 텐서를 생성한다.\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1) # l1의 값을 복사하여 새로운 텐서를 만든다.\n",
    "\n",
    "# 리스트 l2를 사용해 torch.tensor로 텐서를 생성한다.\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "# 리스트 l3를 사용해 torch.as_tensor로 텐서를 생성한다.\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) # 리스트가 복사되는 것임으로 l3의 변경이 t3에게 영향을 미치지 않는다.\n",
    "\n",
    "# 리스트 값을 변경한다.\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "# 각 텐서를 출력한다. 각 값은 변하지 않는다.\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "# NumPy 배열 l4를 사용해 torch.Tensor로 텐서를 생성한다.\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)  # NumPy 배열을 복사하여 새로운 텐서를 만든다.\n",
    "\n",
    "# NumPy 배열 l5를 사용해 torch.tensor로 텐서를 생성한다.\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)  # NumPy 배열을 복사하여 새로운 텐서를 만든다.\n",
    "\n",
    "# NumPy 배열 l6를 사용해 torch.as_tensor로 텐서를 생성한다.\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)  # NumPy 배열과 메모리를 공유하고, l6의 변경이 t6에 영향을 준다.\n",
    "\n",
    "# NumPy 배열 값을 변경한다.\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "# 각 텐서를 출력한다.\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6) # t4와 t5와 달리 torch.as_tensor로 생성된 텐서는 l6와 메모리를 공유하기 때문에 값이 변경된다.\n"
   ],
   "id": "234faa01c88ebf00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "c_tensor_initialization_constant_values",
   "id": "3b256550b2ff7978"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:11:24.541503Z",
     "start_time": "2024-09-23T06:11:24.513920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 크기 (5,)인 1로 채워진 텐서를 생성한다.\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "# t1과 같은 크기와 자료형을 가지는 1로 채워진 텐서를 생성한다.\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "# 크기 (6,)인 0으로 채워진 텐서를 생성한다.\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "# t2와 같은 크기와 자료형을 가지는 0으로 채워진 텐서를 생성한다.\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "# 크기 (4,)인 텐서를 생성한다.\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "# t3와 같은 크기와 자료형을 가지는 텐서를 생성한다.\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "# 초기화 되지 않은 값이 텐서에 들어갈 수 있다.\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "# 3x3 크기의 단위 행렬을 생성한다.\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)"
   ],
   "id": "1cb855a98cd37393",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0000, 1.3828, 0.0000, 0.0000])\n",
      "tensor([1., nan, 3., 0.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "d_tensor_initialization_random_values",
   "id": "c56f80aca2ea06af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:11:36.102011Z",
     "start_time": "2024-09-23T06:11:36.059152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 지정된 범위에서 임의의 정수로 텐서 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "# (1,3) 크기의 텐서를 생성하고, 0과 1사이의 균등 분포에서 값을 샘플링한다.\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "# (1,3) 크기의 텐서를 생성하고, 표준 정규 분포(N(0,1))에서 값을 샘플링한다.\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "# (3,2) 크기의 텐서를 생성하고, 평균이 10.0, 표준편차가 1.0인 정규 분포에서 샘플링한다.\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "# 0.0에서 5.0까지 3개의 값을 균등하게 나눈 텐서를 생성한다.\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "# 0부터 4까지의 정수로 이루어진 텐서를 생성한다.\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 시드를 설정하여 랜덤 숫자를 만들어낸다.\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "# 시드를 재설정하여 랜덤 숫자를 만들어낸다.\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ],
   "id": "8500929b8644a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17, 12]])\n",
      "tensor([[0.0453, 0.5035, 0.9978]])\n",
      "tensor([[0.7419, 0.5923, 0.2908]])\n",
      "tensor([[ 9.2270, 10.0961],\n",
      "        [ 9.4067,  9.5878],\n",
      "        [10.0763, 11.1161]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* torch.manual_seed()는 랜덤 시드를 고정하여 랜덤 숫자가 동일할 수 있도록 만들어준다.",
   "id": "c29da9d7c6f8be7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "e_tensor_type_conversion",
   "id": "e0cbf5bc575efcd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:13:48.829838Z",
     "start_time": "2024-09-23T06:13:48.803390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "# (2,3) 크기의 텐서를 생성하고, 0고 1 사이의 균등 분포에서 값을 샘플링한 후 20을 곱한다.\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "# 텐서 b의 데이터 타입을 int32로 변환한다.\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "# (10, 2) 크기의 1로 채워진 텐서를 생성하고, 데이터 타입을 float64로 설정한다.\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "# (1, 2) 크기의 텐서를 생성하고, 데이터 타입을 int16으로 설정한다.\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "# (10,2) 크기의 0으로 채워진 텐서를 생성하고 데이터 타입을 float64로 설정한다.\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "# (10, 2) 크기의 1로 채워진 텐서를 생성하고, 데이터 타입을 int16으로 설정한다.\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "# (10,2) 크기의 0으로 채워진 텐서를 생성하고, 데이터 타입을 float64로 설정한다.\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "# (10, 2) 크기의 1로 채워진 텐서를 생성하고, 데이터 타입을 int16으로 설정한다.\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "# (10, 2) 크기의 0으로 채워진 텐서를 생성하고, 데이터 타입을 float64로 설정한다.\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "# (10, 2) 크기의 1로 채워진 텐서를 생성하고, 데이터 타입을 int16으로 설정한다.\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "# (5, ) 크기의 표준 정규 분표에서 샘플링한 값을 가지는 텐서를 생성하고 데이터 타입을 float64로 설정한다.\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "# 텐서 double_f를 inst16으로 변환한다.\n",
    "short_g = double_f.to(torch.short)\n",
    "# double_f와 short_g의 곱의 데이터 타입을 출력한다.\n",
    "print((double_f * short_g).dtype)\n"
   ],
   "id": "156c04d54ebf85d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[11.6692, 18.3283,  0.2118],\n",
      "        [18.4972,  9.8370,  3.8937]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "f_tensor_type_conversion",
   "id": "8c4df3103eed835d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:20:15.400607Z",
     "start_time": "2024-09-23T06:20:15.357655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# (2, 3) 크기의 1로 채워진 텐서를 생성한다.\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "# 두 텐서를 더한다.\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 뺀다.\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 곱한다.\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "# 두 텐서를 나눈다.\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ],
   "id": "aebfd5478360d0b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* torch.add() (sub, mul, div)는 연산자로 대체할 수 있다.",
   "id": "808397890aa7fe31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "g_tensor_operations_mm",
   "id": "833d80592925b2da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T06:28:25.314969Z",
     "start_time": "2024-09-23T06:28:25.276263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# [2,3]인 1차원 텐서와 [2,1]인 1차원 텐서를 내적 계산한다.\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "# 정규 분포에서 무작위로 추출한 난수를 생성하여 (2, 3) 크기인 텐서를 반환한다.\n",
    "t2 = torch.randn(2, 3)\n",
    "# 정규 분포에서 무작위로 추출한 난수를 생성하여 (3, 2) 크기인 텐서를 반환한다.\n",
    "t3 = torch.randn(3, 2)\n",
    "# t2와 t3를 내적 계산한다.\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "# 정규 분포에서 무작위로 추출한 난수를 생성하여 (10, 3, 4) 크기인 텐서를 반환한다.\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "# 정규 분포에서 무작위로 추출한 난수를 생성하여 (10, 4, 5) 크기인 텐서를 반환한다.\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "# t5와 t6를 내적 계산한다.\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())"
   ],
   "id": "9e76e2ea785a1bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[-4.9983, -1.9715],\n",
      "        [-2.0698,  0.6196]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "h_tensor_operations_matmul",
   "id": "871275fba91292b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T17:47:37.170205Z",
     "start_time": "2024-09-23T17:47:37.127163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "# 크기가 3인 1차원 텐서 2개를 생성한다.\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "# 텐서를 내적한 후 텐서의 사이즈를 출력한다.\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4) # 크기 [3,4]의 2차원 텐서를 생성한다.\n",
    "t4 = torch.randn(4) # 크기가 4인 1차원 텐서를 생성한다.\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4) # 크기 [10, 3, 4]의 3차원 텐서를 생성한다.\n",
    "t6 = torch.randn(4) # 크기 4의 1차원 텐서를 생성한다.\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4) # 크기 [10, 3, 4]의 3차원 텐서를 생성한다.\n",
    "t8 = torch.randn(10, 4, 5) # 크기 [10, 4, 5]의 3차원 텐서를 생성한다.\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4) # 크기 [10, 3, 4]의 3차원 텐서를 생성한다.\n",
    "t10 = torch.randn(4, 5) # 크기 [4, 5]의 2차원 텐서를 생성한다.\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
   ],
   "id": "a006e22ca0f9c14a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* torch.matmul()은 torch.mm()과 달리 broadcasting이 가능하다.",
   "id": "e5f2a3130e93ed28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "i_tensor_broadcasting",
   "id": "63984fd54b5feada"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T17:50:02.894297Z",
     "start_time": "2024-09-23T17:50:02.851518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# t1과 t2를 곱한다. (broadcasting)\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# t3에서 t4를 뺀다.\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4) # t4가 각 행에 브로드캐스트된다.\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# t5에 대하여 사칙연산을 한다.\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# 각 원소를 255로 나눠 정규화한다.\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "# 3개의 채널을 가진 28x28의 텐서를 생성한다.\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "# t6을 정규화한 후의 크기를 출력한다.\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "# 크기 4의 텐서에 5를 곱한다.\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "# 각 원소를 제곱한다.\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp) # 각 원소를 대응되는 지수로 거듭제곱한다.\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ],
   "id": "efd9b9ad72ba263a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "j_tensosr_indexing_slicing",
   "id": "daa129ae936d75a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T17:57:51.032418Z",
     "start_time": "2024-09-23T17:57:50.993821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 3x5 크기의 텐서를 생성한다.\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9]) / 첫 번째 행\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11]) / 두 번째 열의 모든 값\n",
    "print(x[1, 2])  # >>> tensor(7) / 두 번째 행, 세 번째 열의 값\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14) / 마지막 열의 모든 값\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]]) / 첫 번째 행 제외하고 나머지\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]]) / 첫 번째 행 제외하고 3번째 열부터 마지막 열까지\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6)) # 6x6 텐서를 0으로 초기화한다.\n",
    "y[1:4, 2] = 1 # 2번째부터 4번째 행, 3번째 열에 1을 할당한다.\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) # 2~4번째 행과 2~4번째 열\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# 3x4 크기의 텐서를 생성한다.\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2]) # 1번째와 2번째 행\n",
    "print(z[1:, 1:3]) # 1번째 행 제외하고 2번째 열과 3번째 열\n",
    "print(z[:, 1:]) # 모든 행에 대해서 2번째 열부터 마지막 열\n",
    "\n",
    "z[1:, 1:3] = 0 # 두번째 행부터 마지막 행까지의 1번쨰와 2번째 열을 0으로 설정한다.\n",
    "print(z)"
   ],
   "id": "6ef824df5618dd07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "k_tensor_reshaping",
   "id": "1d5c09ca90907551"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:03:31.044911Z",
     "start_time": "2024-09-23T18:03:30.995576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 2x3 크기의 텐서 생성\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# t1의 크기를 (3,2)로 변경\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "# t1의 크기를 (1,6)으로 변경\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# 0부터 7까지의 값을 가지는 텐서를 (2,4) 크기로 변경\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "# 0부터 5까지의 값을 가지는 텐서를 (2,3) 크기로 변경\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "# 1번째 차원에 새로운 차원 추가\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor / 텐서를 일차원으로 펼치기\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "# 1번째 차원 이후부터 펼치기\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "# 텐서 차원 재배열\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions / 차원을 재배열 (dims=(0, 1)은 원래 차원을 유지)\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "# 차원을 재배열 (0번째와 1번째 차원을 교환)\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "# torch.t() = 행과 열을 교환하는 전치 함수\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ],
   "id": "fbafa822648eb166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "l_tensosr_concat",
   "id": "5a5b820f897d0d57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:09:41.535619Z",
     "start_time": "2024-09-23T18:09:41.514830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "# 1번째 차원을 기준으로 t1, t2, t3를 연결한다.\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "# 0번째 차원을 기준으로 t5, t6를 연결한다.\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ],
   "id": "879acd1d3d3fab93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "m_tensor_stacking",
   "id": "bb40052894b3dab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:12:43.616611Z",
     "start_time": "2024-09-23T18:12:43.598665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 0번째 차원에서 두 텐서를 쌓는다.\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0) # unsqueeze를 통해 차원을 확장한다.\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "# 1번째 차원에서 두 텐서를 쌓는다.\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "# 2번째 차원에서 두 텐서를 쌓는ㄷ나.\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ],
   "id": "f0f2498a2461dfdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* torch.stack()은 새 차원을 추가하면서 텐서를 쌓는다.\n",
    "* torch.cat()은 차원을 추가하지 않고 기존 차원에서 텐서를 이어붙인다.\n",
    "* cat을 사용할 때는 unsqueeze()를 사용하여 차원을 확장한다."
   ],
   "id": "a2e2592777c92167"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "n_tensor_vstack_hstack",
   "id": "40d839fca9289d99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:19:12.697418Z",
     "start_time": "2024-09-23T18:19:12.659946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 1차원 텐서 2개 생성\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "# 두 텐서를 수직으로 결합해 2차원 텐서 생성\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "# 2차원 텐서 2개 생성\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "\n",
    "# 수직으로 결합하여 6x1 텐서 생성\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# (2, 2, 3) 크기의 3차원 텐서를 수직으로 결합 -> 첫 번째 차원 확장\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "# 두 텐서를 수평으로 결합해 1차원 텐서 생성\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "# 두 텐서를 수평으로 결합해 각 행에 두 열이 추가된 텐서 생성\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# 두 텐서를 수평으로 결합 -> 두 번째 차원 확장\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ],
   "id": "b87de8aa1091fcba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* torch.vstack(): 텐서를 수직으로 쌓는다. 1차원, 2차원 텐서를 수직으로 결합하여 새로운 차원을 형성한다.\n",
    "* torch.hstack(): 텐서를 수평으로 쌓는다. 같은 차원에서 텐서의 열을 결합한다."
   ],
   "id": "7d6443e231655a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "__느낀점__\n",
    "이번 과제를 통해 PyTorch의 텐서 초기화 방법과 다양한 연산에 대해 깊이 있게 학습할 수 있었다. torch.Tensor와 torch.tensor의 차이, 그리고 torch.as_tensor의 활용법을 이해할 수 있었다. 리스트나 넘파이 배열을 텐서로 변환할 때 메모리를 공유할지, 값을 복사할지에 따라 사용하는 함수가 다르다는 것을 알 수 있었다. 브로드캐스팅 기능과 행렬 연산을 pytorch에서 사용하는 방법을 알 수 있었다. 또한, 파이썬 문법을 다시 한 번 공부해야 겠다는 생각이 들었다."
   ],
   "id": "dbf465a11010ffe0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
